{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qsvy0pBiFFSW"
      },
      "outputs": [],
      "source": [
        "# In this project we are going to learn about the LSTM architecture to build an Chatbot about predict-\n",
        "# ing the sequence of word\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\"Grit is a unique combination of passion and perseverance, more important than talent in achieving long-term success.\n",
        " While talent matters, effort is crucial as it factors in twice: it builds skill and makes skill productive. People with grit focus on long-term goals and maintain determination despite failures and setbacks.\n",
        "Passion is not something innate but develops through discovery, practice, and a lifetime of refinement. It involves caring deeply about a singular, top-level goal that gives meaning and direction to all other goals. Enthusiasm is common, but sustained commitment is rare. Grit requires working on something you deeply care about over a long period, staying loyal to the goal through challenges.\n",
        "Deliberate practice is a hallmark of grit. It involves setting stretch goals, focusing on areas of weakness, seeking immediate feedback, and refining skills through repetition. Experts practice differently than novices, devoting thousands of hours to deliberate improvement. Flow, a state of effortless focus, is an enjoyable complement to deliberate practice but typically occurs during performance rather than preparation.\n",
        "A growth mindset fuels grit by fostering the belief that abilities can improve with effort. Optimistic self-talk and resilience in the face of adversity are key components of grit. Gritty people view setbacks as temporary and specific rather than permanent and pervasive. They believe in their capacity to overcome challenges through consistent effort.\n",
        "Purpose is a critical element of grit, connecting personal goals to contributions that benefit others. People with grit often see their work as meaningful beyond themselves, enhancing motivation. Purpose can be cultivated by reflecting on how one’s work helps others, aligning tasks with core values, and finding inspiration in role models.\n",
        "Grit also thrives in environments that support it. A gritty culture promotes values of perseverance, hard work, and dedication. Joining such a culture can help internalize these values, reinforcing grit as a personal trait. Leaders can foster grit in organizations by setting examples and establishing norms that encourage passion and persistence.\n",
        "Teaching grit to children involves encouraging them to pursue long-term interests and stick with activities for more than a year. Overbearing parenting erodes intrinsic motivation, while supportive environments help kids discover passions. Extracurricular activities, pursued consistently, build grit and correlate with greater success in adulthood.\n",
        "Effort over time is the defining characteristic of grit. Many people quit too early or shift focus before reaching their potential. Grit is about staying the course, even when progress is slow or results are not immediate. It requires commitment to a singular life philosophy, with lower-level goals serving as stepping stones to the ultimate aim.\n",
        "Angela Duckworth emphasizes that gritty individuals make decisions aligned with their identity and long-term aspirations. They prioritize goals that align with their ultimate concerns and are willing to adjust lower-level goals if needed. Grit grows with maturity, as individuals learn from experiences, adapt to challenges, and solidify their sense of purpose.\n",
        "Ultimately, grit enables people to achieve extraordinary things through consistent effort and a refusal to give up. It is a skill that can be cultivated, strengthened, and applied to all areas of life, providing a roadmap for success through passion and perseverance.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qF-7PI_hHKgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n"
      ],
      "metadata": {
        "id": "kpxwH7yhI_O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([text])\n",
        "print(tokenizer.word_index)\n",
        "vocab_size=len(tokenizer.word_index)+1\n",
        "print(vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJBrb6j3JDu_",
        "outputId": "27401867-1cfa-4de5-e0bf-95b8f35dc491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'and': 1, 'grit': 2, 'a': 3, 'to': 4, 'of': 5, 'is': 6, 'with': 7, 'in': 8, 'that': 9, 'it': 10, 'goals': 11, 'as': 12, 'through': 13, 'the': 14, 'their': 15, 'than': 16, 'long': 17, 'effort': 18, 'people': 19, 'can': 20, 'passion': 21, 'term': 22, 'on': 23, 'practice': 24, 'perseverance': 25, 'success': 26, 'skill': 27, 'focus': 28, 'but': 29, 'involves': 30, 'about': 31, 'level': 32, 'challenges': 33, 'deliberate': 34, 'by': 35, 'are': 36, 'gritty': 37, 'purpose': 38, 'work': 39, 'values': 40, 'more': 41, 'talent': 42, 'while': 43, 'setbacks': 44, 'not': 45, 'something': 46, 'deeply': 47, 'singular': 48, 'goal': 49, 'all': 50, 'commitment': 51, 'requires': 52, 'over': 53, 'staying': 54, 'setting': 55, 'areas': 56, 'immediate': 57, 'rather': 58, 'they': 59, 'consistent': 60, 'personal': 61, 'others': 62, 'motivation': 63, 'be': 64, 'cultivated': 65, 'environments': 66, 'culture': 67, 'help': 68, 'activities': 69, 'for': 70, 'or': 71, 'life': 72, 'lower': 73, 'ultimate': 74, 'individuals': 75, 'unique': 76, 'combination': 77, 'important': 78, 'achieving': 79, 'matters': 80, 'crucial': 81, 'factors': 82, 'twice': 83, 'builds': 84, 'makes': 85, 'productive': 86, 'maintain': 87, 'determination': 88, 'despite': 89, 'failures': 90, 'innate': 91, 'develops': 92, 'discovery': 93, 'lifetime': 94, 'refinement': 95, 'caring': 96, 'top': 97, 'gives': 98, 'meaning': 99, 'direction': 100, 'other': 101, 'enthusiasm': 102, 'common': 103, 'sustained': 104, 'rare': 105, 'working': 106, 'you': 107, 'care': 108, 'period': 109, 'loyal': 110, 'hallmark': 111, 'stretch': 112, 'focusing': 113, 'weakness': 114, 'seeking': 115, 'feedback': 116, 'refining': 117, 'skills': 118, 'repetition': 119, 'experts': 120, 'differently': 121, 'novices': 122, 'devoting': 123, 'thousands': 124, 'hours': 125, 'improvement': 126, 'flow': 127, 'state': 128, 'effortless': 129, 'an': 130, 'enjoyable': 131, 'complement': 132, 'typically': 133, 'occurs': 134, 'during': 135, 'performance': 136, 'preparation': 137, 'growth': 138, 'mindset': 139, 'fuels': 140, 'fostering': 141, 'belief': 142, 'abilities': 143, 'improve': 144, 'optimistic': 145, 'self': 146, 'talk': 147, 'resilience': 148, 'face': 149, 'adversity': 150, 'key': 151, 'components': 152, 'view': 153, 'temporary': 154, 'specific': 155, 'permanent': 156, 'pervasive': 157, 'believe': 158, 'capacity': 159, 'overcome': 160, 'critical': 161, 'element': 162, 'connecting': 163, 'contributions': 164, 'benefit': 165, 'often': 166, 'see': 167, 'meaningful': 168, 'beyond': 169, 'themselves': 170, 'enhancing': 171, 'reflecting': 172, 'how': 173, 'one’s': 174, 'helps': 175, 'aligning': 176, 'tasks': 177, 'core': 178, 'finding': 179, 'inspiration': 180, 'role': 181, 'models': 182, 'also': 183, 'thrives': 184, 'support': 185, 'promotes': 186, 'hard': 187, 'dedication': 188, 'joining': 189, 'such': 190, 'internalize': 191, 'these': 192, 'reinforcing': 193, 'trait': 194, 'leaders': 195, 'foster': 196, 'organizations': 197, 'examples': 198, 'establishing': 199, 'norms': 200, 'encourage': 201, 'persistence': 202, 'teaching': 203, 'children': 204, 'encouraging': 205, 'them': 206, 'pursue': 207, 'interests': 208, 'stick': 209, 'year': 210, 'overbearing': 211, 'parenting': 212, 'erodes': 213, 'intrinsic': 214, 'supportive': 215, 'kids': 216, 'discover': 217, 'passions': 218, 'extracurricular': 219, 'pursued': 220, 'consistently': 221, 'build': 222, 'correlate': 223, 'greater': 224, 'adulthood': 225, 'time': 226, 'defining': 227, 'characteristic': 228, 'many': 229, 'quit': 230, 'too': 231, 'early': 232, 'shift': 233, 'before': 234, 'reaching': 235, 'potential': 236, 'course': 237, 'even': 238, 'when': 239, 'progress': 240, 'slow': 241, 'results': 242, 'philosophy': 243, 'serving': 244, 'stepping': 245, 'stones': 246, 'aim': 247, 'angela': 248, 'duckworth': 249, 'emphasizes': 250, 'make': 251, 'decisions': 252, 'aligned': 253, 'identity': 254, 'aspirations': 255, 'prioritize': 256, 'align': 257, 'concerns': 258, 'willing': 259, 'adjust': 260, 'if': 261, 'needed': 262, 'grows': 263, 'maturity': 264, 'learn': 265, 'from': 266, 'experiences': 267, 'adapt': 268, 'solidify': 269, 'sense': 270, 'ultimately': 271, 'enables': 272, 'achieve': 273, 'extraordinary': 274, 'things': 275, 'refusal': 276, 'give': 277, 'up': 278, 'strengthened': 279, 'applied': 280, 'providing': 281, 'roadmap': 282}\n",
            "283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PZOMlMwtKZ76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq=[]\n",
        "for line in text.split('.'):\n",
        "    token_list=tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1,len(token_list)):\n",
        "        n_gram_sequence=token_list[:i+1]\n",
        "        input_seq.append(n_gram_sequence)\n",
        "print(input_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvYSsbD3JKwL",
        "outputId": "5aa75dff-85bf-4ed3-d479-75b32dd22c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 6], [2, 6, 3], [2, 6, 3, 76], [2, 6, 3, 76, 77], [2, 6, 3, 76, 77, 5], [2, 6, 3, 76, 77, 5, 21], [2, 6, 3, 76, 77, 5, 21, 1], [2, 6, 3, 76, 77, 5, 21, 1, 25], [2, 6, 3, 76, 77, 5, 21, 1, 25, 41], [2, 6, 3, 76, 77, 5, 21, 1, 25, 41, 78], [2, 6, 3, 76, 77, 5, 21, 1, 25, 41, 78, 16], [2, 6, 3, 76, 77, 5, 21, 1, 25, 41, 78, 16, 42], [2, 6, 3, 76, 77, 5, 21, 1, 25, 41, 78, 16, 42, 8], [2, 6, 3, 76, 77, 5, 21, 1, 25, 41, 78, 16, 42, 8, 79], [2, 6, 3, 76, 77, 5, 21, 1, 25, 41, 78, 16, 42, 8, 79, 17], [2, 6, 3, 76, 77, 5, 21, 1, 25, 41, 78, 16, 42, 8, 79, 17, 22], [2, 6, 3, 76, 77, 5, 21, 1, 25, 41, 78, 16, 42, 8, 79, 17, 22, 26], [43, 42], [43, 42, 80], [43, 42, 80, 18], [43, 42, 80, 18, 6], [43, 42, 80, 18, 6, 81], [43, 42, 80, 18, 6, 81, 12], [43, 42, 80, 18, 6, 81, 12, 10], [43, 42, 80, 18, 6, 81, 12, 10, 82], [43, 42, 80, 18, 6, 81, 12, 10, 82, 8], [43, 42, 80, 18, 6, 81, 12, 10, 82, 8, 83], [43, 42, 80, 18, 6, 81, 12, 10, 82, 8, 83, 10], [43, 42, 80, 18, 6, 81, 12, 10, 82, 8, 83, 10, 84], [43, 42, 80, 18, 6, 81, 12, 10, 82, 8, 83, 10, 84, 27], [43, 42, 80, 18, 6, 81, 12, 10, 82, 8, 83, 10, 84, 27, 1], [43, 42, 80, 18, 6, 81, 12, 10, 82, 8, 83, 10, 84, 27, 1, 85], [43, 42, 80, 18, 6, 81, 12, 10, 82, 8, 83, 10, 84, 27, 1, 85, 27], [43, 42, 80, 18, 6, 81, 12, 10, 82, 8, 83, 10, 84, 27, 1, 85, 27, 86], [19, 7], [19, 7, 2], [19, 7, 2, 28], [19, 7, 2, 28, 23], [19, 7, 2, 28, 23, 17], [19, 7, 2, 28, 23, 17, 22], [19, 7, 2, 28, 23, 17, 22, 11], [19, 7, 2, 28, 23, 17, 22, 11, 1], [19, 7, 2, 28, 23, 17, 22, 11, 1, 87], [19, 7, 2, 28, 23, 17, 22, 11, 1, 87, 88], [19, 7, 2, 28, 23, 17, 22, 11, 1, 87, 88, 89], [19, 7, 2, 28, 23, 17, 22, 11, 1, 87, 88, 89, 90], [19, 7, 2, 28, 23, 17, 22, 11, 1, 87, 88, 89, 90, 1], [19, 7, 2, 28, 23, 17, 22, 11, 1, 87, 88, 89, 90, 1, 44], [21, 6], [21, 6, 45], [21, 6, 45, 46], [21, 6, 45, 46, 91], [21, 6, 45, 46, 91, 29], [21, 6, 45, 46, 91, 29, 92], [21, 6, 45, 46, 91, 29, 92, 13], [21, 6, 45, 46, 91, 29, 92, 13, 93], [21, 6, 45, 46, 91, 29, 92, 13, 93, 24], [21, 6, 45, 46, 91, 29, 92, 13, 93, 24, 1], [21, 6, 45, 46, 91, 29, 92, 13, 93, 24, 1, 3], [21, 6, 45, 46, 91, 29, 92, 13, 93, 24, 1, 3, 94], [21, 6, 45, 46, 91, 29, 92, 13, 93, 24, 1, 3, 94, 5], [21, 6, 45, 46, 91, 29, 92, 13, 93, 24, 1, 3, 94, 5, 95], [10, 30], [10, 30, 96], [10, 30, 96, 47], [10, 30, 96, 47, 31], [10, 30, 96, 47, 31, 3], [10, 30, 96, 47, 31, 3, 48], [10, 30, 96, 47, 31, 3, 48, 97], [10, 30, 96, 47, 31, 3, 48, 97, 32], [10, 30, 96, 47, 31, 3, 48, 97, 32, 49], [10, 30, 96, 47, 31, 3, 48, 97, 32, 49, 9], [10, 30, 96, 47, 31, 3, 48, 97, 32, 49, 9, 98], [10, 30, 96, 47, 31, 3, 48, 97, 32, 49, 9, 98, 99], [10, 30, 96, 47, 31, 3, 48, 97, 32, 49, 9, 98, 99, 1], [10, 30, 96, 47, 31, 3, 48, 97, 32, 49, 9, 98, 99, 1, 100], [10, 30, 96, 47, 31, 3, 48, 97, 32, 49, 9, 98, 99, 1, 100, 4], [10, 30, 96, 47, 31, 3, 48, 97, 32, 49, 9, 98, 99, 1, 100, 4, 50], [10, 30, 96, 47, 31, 3, 48, 97, 32, 49, 9, 98, 99, 1, 100, 4, 50, 101], [10, 30, 96, 47, 31, 3, 48, 97, 32, 49, 9, 98, 99, 1, 100, 4, 50, 101, 11], [102, 6], [102, 6, 103], [102, 6, 103, 29], [102, 6, 103, 29, 104], [102, 6, 103, 29, 104, 51], [102, 6, 103, 29, 104, 51, 6], [102, 6, 103, 29, 104, 51, 6, 105], [2, 52], [2, 52, 106], [2, 52, 106, 23], [2, 52, 106, 23, 46], [2, 52, 106, 23, 46, 107], [2, 52, 106, 23, 46, 107, 47], [2, 52, 106, 23, 46, 107, 47, 108], [2, 52, 106, 23, 46, 107, 47, 108, 31], [2, 52, 106, 23, 46, 107, 47, 108, 31, 53], [2, 52, 106, 23, 46, 107, 47, 108, 31, 53, 3], [2, 52, 106, 23, 46, 107, 47, 108, 31, 53, 3, 17], [2, 52, 106, 23, 46, 107, 47, 108, 31, 53, 3, 17, 109], [2, 52, 106, 23, 46, 107, 47, 108, 31, 53, 3, 17, 109, 54], [2, 52, 106, 23, 46, 107, 47, 108, 31, 53, 3, 17, 109, 54, 110], [2, 52, 106, 23, 46, 107, 47, 108, 31, 53, 3, 17, 109, 54, 110, 4], [2, 52, 106, 23, 46, 107, 47, 108, 31, 53, 3, 17, 109, 54, 110, 4, 14], [2, 52, 106, 23, 46, 107, 47, 108, 31, 53, 3, 17, 109, 54, 110, 4, 14, 49], [2, 52, 106, 23, 46, 107, 47, 108, 31, 53, 3, 17, 109, 54, 110, 4, 14, 49, 13], [2, 52, 106, 23, 46, 107, 47, 108, 31, 53, 3, 17, 109, 54, 110, 4, 14, 49, 13, 33], [34, 24], [34, 24, 6], [34, 24, 6, 3], [34, 24, 6, 3, 111], [34, 24, 6, 3, 111, 5], [34, 24, 6, 3, 111, 5, 2], [10, 30], [10, 30, 55], [10, 30, 55, 112], [10, 30, 55, 112, 11], [10, 30, 55, 112, 11, 113], [10, 30, 55, 112, 11, 113, 23], [10, 30, 55, 112, 11, 113, 23, 56], [10, 30, 55, 112, 11, 113, 23, 56, 5], [10, 30, 55, 112, 11, 113, 23, 56, 5, 114], [10, 30, 55, 112, 11, 113, 23, 56, 5, 114, 115], [10, 30, 55, 112, 11, 113, 23, 56, 5, 114, 115, 57], [10, 30, 55, 112, 11, 113, 23, 56, 5, 114, 115, 57, 116], [10, 30, 55, 112, 11, 113, 23, 56, 5, 114, 115, 57, 116, 1], [10, 30, 55, 112, 11, 113, 23, 56, 5, 114, 115, 57, 116, 1, 117], [10, 30, 55, 112, 11, 113, 23, 56, 5, 114, 115, 57, 116, 1, 117, 118], [10, 30, 55, 112, 11, 113, 23, 56, 5, 114, 115, 57, 116, 1, 117, 118, 13], [10, 30, 55, 112, 11, 113, 23, 56, 5, 114, 115, 57, 116, 1, 117, 118, 13, 119], [120, 24], [120, 24, 121], [120, 24, 121, 16], [120, 24, 121, 16, 122], [120, 24, 121, 16, 122, 123], [120, 24, 121, 16, 122, 123, 124], [120, 24, 121, 16, 122, 123, 124, 5], [120, 24, 121, 16, 122, 123, 124, 5, 125], [120, 24, 121, 16, 122, 123, 124, 5, 125, 4], [120, 24, 121, 16, 122, 123, 124, 5, 125, 4, 34], [120, 24, 121, 16, 122, 123, 124, 5, 125, 4, 34, 126], [127, 3], [127, 3, 128], [127, 3, 128, 5], [127, 3, 128, 5, 129], [127, 3, 128, 5, 129, 28], [127, 3, 128, 5, 129, 28, 6], [127, 3, 128, 5, 129, 28, 6, 130], [127, 3, 128, 5, 129, 28, 6, 130, 131], [127, 3, 128, 5, 129, 28, 6, 130, 131, 132], [127, 3, 128, 5, 129, 28, 6, 130, 131, 132, 4], [127, 3, 128, 5, 129, 28, 6, 130, 131, 132, 4, 34], [127, 3, 128, 5, 129, 28, 6, 130, 131, 132, 4, 34, 24], [127, 3, 128, 5, 129, 28, 6, 130, 131, 132, 4, 34, 24, 29], [127, 3, 128, 5, 129, 28, 6, 130, 131, 132, 4, 34, 24, 29, 133], [127, 3, 128, 5, 129, 28, 6, 130, 131, 132, 4, 34, 24, 29, 133, 134], [127, 3, 128, 5, 129, 28, 6, 130, 131, 132, 4, 34, 24, 29, 133, 134, 135], [127, 3, 128, 5, 129, 28, 6, 130, 131, 132, 4, 34, 24, 29, 133, 134, 135, 136], [127, 3, 128, 5, 129, 28, 6, 130, 131, 132, 4, 34, 24, 29, 133, 134, 135, 136, 58], [127, 3, 128, 5, 129, 28, 6, 130, 131, 132, 4, 34, 24, 29, 133, 134, 135, 136, 58, 16], [127, 3, 128, 5, 129, 28, 6, 130, 131, 132, 4, 34, 24, 29, 133, 134, 135, 136, 58, 16, 137], [3, 138], [3, 138, 139], [3, 138, 139, 140], [3, 138, 139, 140, 2], [3, 138, 139, 140, 2, 35], [3, 138, 139, 140, 2, 35, 141], [3, 138, 139, 140, 2, 35, 141, 14], [3, 138, 139, 140, 2, 35, 141, 14, 142], [3, 138, 139, 140, 2, 35, 141, 14, 142, 9], [3, 138, 139, 140, 2, 35, 141, 14, 142, 9, 143], [3, 138, 139, 140, 2, 35, 141, 14, 142, 9, 143, 20], [3, 138, 139, 140, 2, 35, 141, 14, 142, 9, 143, 20, 144], [3, 138, 139, 140, 2, 35, 141, 14, 142, 9, 143, 20, 144, 7], [3, 138, 139, 140, 2, 35, 141, 14, 142, 9, 143, 20, 144, 7, 18], [145, 146], [145, 146, 147], [145, 146, 147, 1], [145, 146, 147, 1, 148], [145, 146, 147, 1, 148, 8], [145, 146, 147, 1, 148, 8, 14], [145, 146, 147, 1, 148, 8, 14, 149], [145, 146, 147, 1, 148, 8, 14, 149, 5], [145, 146, 147, 1, 148, 8, 14, 149, 5, 150], [145, 146, 147, 1, 148, 8, 14, 149, 5, 150, 36], [145, 146, 147, 1, 148, 8, 14, 149, 5, 150, 36, 151], [145, 146, 147, 1, 148, 8, 14, 149, 5, 150, 36, 151, 152], [145, 146, 147, 1, 148, 8, 14, 149, 5, 150, 36, 151, 152, 5], [145, 146, 147, 1, 148, 8, 14, 149, 5, 150, 36, 151, 152, 5, 2], [37, 19], [37, 19, 153], [37, 19, 153, 44], [37, 19, 153, 44, 12], [37, 19, 153, 44, 12, 154], [37, 19, 153, 44, 12, 154, 1], [37, 19, 153, 44, 12, 154, 1, 155], [37, 19, 153, 44, 12, 154, 1, 155, 58], [37, 19, 153, 44, 12, 154, 1, 155, 58, 16], [37, 19, 153, 44, 12, 154, 1, 155, 58, 16, 156], [37, 19, 153, 44, 12, 154, 1, 155, 58, 16, 156, 1], [37, 19, 153, 44, 12, 154, 1, 155, 58, 16, 156, 1, 157], [59, 158], [59, 158, 8], [59, 158, 8, 15], [59, 158, 8, 15, 159], [59, 158, 8, 15, 159, 4], [59, 158, 8, 15, 159, 4, 160], [59, 158, 8, 15, 159, 4, 160, 33], [59, 158, 8, 15, 159, 4, 160, 33, 13], [59, 158, 8, 15, 159, 4, 160, 33, 13, 60], [59, 158, 8, 15, 159, 4, 160, 33, 13, 60, 18], [38, 6], [38, 6, 3], [38, 6, 3, 161], [38, 6, 3, 161, 162], [38, 6, 3, 161, 162, 5], [38, 6, 3, 161, 162, 5, 2], [38, 6, 3, 161, 162, 5, 2, 163], [38, 6, 3, 161, 162, 5, 2, 163, 61], [38, 6, 3, 161, 162, 5, 2, 163, 61, 11], [38, 6, 3, 161, 162, 5, 2, 163, 61, 11, 4], [38, 6, 3, 161, 162, 5, 2, 163, 61, 11, 4, 164], [38, 6, 3, 161, 162, 5, 2, 163, 61, 11, 4, 164, 9], [38, 6, 3, 161, 162, 5, 2, 163, 61, 11, 4, 164, 9, 165], [38, 6, 3, 161, 162, 5, 2, 163, 61, 11, 4, 164, 9, 165, 62], [19, 7], [19, 7, 2], [19, 7, 2, 166], [19, 7, 2, 166, 167], [19, 7, 2, 166, 167, 15], [19, 7, 2, 166, 167, 15, 39], [19, 7, 2, 166, 167, 15, 39, 12], [19, 7, 2, 166, 167, 15, 39, 12, 168], [19, 7, 2, 166, 167, 15, 39, 12, 168, 169], [19, 7, 2, 166, 167, 15, 39, 12, 168, 169, 170], [19, 7, 2, 166, 167, 15, 39, 12, 168, 169, 170, 171], [19, 7, 2, 166, 167, 15, 39, 12, 168, 169, 170, 171, 63], [38, 20], [38, 20, 64], [38, 20, 64, 65], [38, 20, 64, 65, 35], [38, 20, 64, 65, 35, 172], [38, 20, 64, 65, 35, 172, 23], [38, 20, 64, 65, 35, 172, 23, 173], [38, 20, 64, 65, 35, 172, 23, 173, 174], [38, 20, 64, 65, 35, 172, 23, 173, 174, 39], [38, 20, 64, 65, 35, 172, 23, 173, 174, 39, 175], [38, 20, 64, 65, 35, 172, 23, 173, 174, 39, 175, 62], [38, 20, 64, 65, 35, 172, 23, 173, 174, 39, 175, 62, 176], [38, 20, 64, 65, 35, 172, 23, 173, 174, 39, 175, 62, 176, 177], [38, 20, 64, 65, 35, 172, 23, 173, 174, 39, 175, 62, 176, 177, 7], [38, 20, 64, 65, 35, 172, 23, 173, 174, 39, 175, 62, 176, 177, 7, 178], [38, 20, 64, 65, 35, 172, 23, 173, 174, 39, 175, 62, 176, 177, 7, 178, 40], [38, 20, 64, 65, 35, 172, 23, 173, 174, 39, 175, 62, 176, 177, 7, 178, 40, 1], [38, 20, 64, 65, 35, 172, 23, 173, 174, 39, 175, 62, 176, 177, 7, 178, 40, 1, 179], [38, 20, 64, 65, 35, 172, 23, 173, 174, 39, 175, 62, 176, 177, 7, 178, 40, 1, 179, 180], [38, 20, 64, 65, 35, 172, 23, 173, 174, 39, 175, 62, 176, 177, 7, 178, 40, 1, 179, 180, 8], [38, 20, 64, 65, 35, 172, 23, 173, 174, 39, 175, 62, 176, 177, 7, 178, 40, 1, 179, 180, 8, 181], [38, 20, 64, 65, 35, 172, 23, 173, 174, 39, 175, 62, 176, 177, 7, 178, 40, 1, 179, 180, 8, 181, 182], [2, 183], [2, 183, 184], [2, 183, 184, 8], [2, 183, 184, 8, 66], [2, 183, 184, 8, 66, 9], [2, 183, 184, 8, 66, 9, 185], [2, 183, 184, 8, 66, 9, 185, 10], [3, 37], [3, 37, 67], [3, 37, 67, 186], [3, 37, 67, 186, 40], [3, 37, 67, 186, 40, 5], [3, 37, 67, 186, 40, 5, 25], [3, 37, 67, 186, 40, 5, 25, 187], [3, 37, 67, 186, 40, 5, 25, 187, 39], [3, 37, 67, 186, 40, 5, 25, 187, 39, 1], [3, 37, 67, 186, 40, 5, 25, 187, 39, 1, 188], [189, 190], [189, 190, 3], [189, 190, 3, 67], [189, 190, 3, 67, 20], [189, 190, 3, 67, 20, 68], [189, 190, 3, 67, 20, 68, 191], [189, 190, 3, 67, 20, 68, 191, 192], [189, 190, 3, 67, 20, 68, 191, 192, 40], [189, 190, 3, 67, 20, 68, 191, 192, 40, 193], [189, 190, 3, 67, 20, 68, 191, 192, 40, 193, 2], [189, 190, 3, 67, 20, 68, 191, 192, 40, 193, 2, 12], [189, 190, 3, 67, 20, 68, 191, 192, 40, 193, 2, 12, 3], [189, 190, 3, 67, 20, 68, 191, 192, 40, 193, 2, 12, 3, 61], [189, 190, 3, 67, 20, 68, 191, 192, 40, 193, 2, 12, 3, 61, 194], [195, 20], [195, 20, 196], [195, 20, 196, 2], [195, 20, 196, 2, 8], [195, 20, 196, 2, 8, 197], [195, 20, 196, 2, 8, 197, 35], [195, 20, 196, 2, 8, 197, 35, 55], [195, 20, 196, 2, 8, 197, 35, 55, 198], [195, 20, 196, 2, 8, 197, 35, 55, 198, 1], [195, 20, 196, 2, 8, 197, 35, 55, 198, 1, 199], [195, 20, 196, 2, 8, 197, 35, 55, 198, 1, 199, 200], [195, 20, 196, 2, 8, 197, 35, 55, 198, 1, 199, 200, 9], [195, 20, 196, 2, 8, 197, 35, 55, 198, 1, 199, 200, 9, 201], [195, 20, 196, 2, 8, 197, 35, 55, 198, 1, 199, 200, 9, 201, 21], [195, 20, 196, 2, 8, 197, 35, 55, 198, 1, 199, 200, 9, 201, 21, 1], [195, 20, 196, 2, 8, 197, 35, 55, 198, 1, 199, 200, 9, 201, 21, 1, 202], [203, 2], [203, 2, 4], [203, 2, 4, 204], [203, 2, 4, 204, 30], [203, 2, 4, 204, 30, 205], [203, 2, 4, 204, 30, 205, 206], [203, 2, 4, 204, 30, 205, 206, 4], [203, 2, 4, 204, 30, 205, 206, 4, 207], [203, 2, 4, 204, 30, 205, 206, 4, 207, 17], [203, 2, 4, 204, 30, 205, 206, 4, 207, 17, 22], [203, 2, 4, 204, 30, 205, 206, 4, 207, 17, 22, 208], [203, 2, 4, 204, 30, 205, 206, 4, 207, 17, 22, 208, 1], [203, 2, 4, 204, 30, 205, 206, 4, 207, 17, 22, 208, 1, 209], [203, 2, 4, 204, 30, 205, 206, 4, 207, 17, 22, 208, 1, 209, 7], [203, 2, 4, 204, 30, 205, 206, 4, 207, 17, 22, 208, 1, 209, 7, 69], [203, 2, 4, 204, 30, 205, 206, 4, 207, 17, 22, 208, 1, 209, 7, 69, 70], [203, 2, 4, 204, 30, 205, 206, 4, 207, 17, 22, 208, 1, 209, 7, 69, 70, 41], [203, 2, 4, 204, 30, 205, 206, 4, 207, 17, 22, 208, 1, 209, 7, 69, 70, 41, 16], [203, 2, 4, 204, 30, 205, 206, 4, 207, 17, 22, 208, 1, 209, 7, 69, 70, 41, 16, 3], [203, 2, 4, 204, 30, 205, 206, 4, 207, 17, 22, 208, 1, 209, 7, 69, 70, 41, 16, 3, 210], [211, 212], [211, 212, 213], [211, 212, 213, 214], [211, 212, 213, 214, 63], [211, 212, 213, 214, 63, 43], [211, 212, 213, 214, 63, 43, 215], [211, 212, 213, 214, 63, 43, 215, 66], [211, 212, 213, 214, 63, 43, 215, 66, 68], [211, 212, 213, 214, 63, 43, 215, 66, 68, 216], [211, 212, 213, 214, 63, 43, 215, 66, 68, 216, 217], [211, 212, 213, 214, 63, 43, 215, 66, 68, 216, 217, 218], [219, 69], [219, 69, 220], [219, 69, 220, 221], [219, 69, 220, 221, 222], [219, 69, 220, 221, 222, 2], [219, 69, 220, 221, 222, 2, 1], [219, 69, 220, 221, 222, 2, 1, 223], [219, 69, 220, 221, 222, 2, 1, 223, 7], [219, 69, 220, 221, 222, 2, 1, 223, 7, 224], [219, 69, 220, 221, 222, 2, 1, 223, 7, 224, 26], [219, 69, 220, 221, 222, 2, 1, 223, 7, 224, 26, 8], [219, 69, 220, 221, 222, 2, 1, 223, 7, 224, 26, 8, 225], [18, 53], [18, 53, 226], [18, 53, 226, 6], [18, 53, 226, 6, 14], [18, 53, 226, 6, 14, 227], [18, 53, 226, 6, 14, 227, 228], [18, 53, 226, 6, 14, 227, 228, 5], [18, 53, 226, 6, 14, 227, 228, 5, 2], [229, 19], [229, 19, 230], [229, 19, 230, 231], [229, 19, 230, 231, 232], [229, 19, 230, 231, 232, 71], [229, 19, 230, 231, 232, 71, 233], [229, 19, 230, 231, 232, 71, 233, 28], [229, 19, 230, 231, 232, 71, 233, 28, 234], [229, 19, 230, 231, 232, 71, 233, 28, 234, 235], [229, 19, 230, 231, 232, 71, 233, 28, 234, 235, 15], [229, 19, 230, 231, 232, 71, 233, 28, 234, 235, 15, 236], [2, 6], [2, 6, 31], [2, 6, 31, 54], [2, 6, 31, 54, 14], [2, 6, 31, 54, 14, 237], [2, 6, 31, 54, 14, 237, 238], [2, 6, 31, 54, 14, 237, 238, 239], [2, 6, 31, 54, 14, 237, 238, 239, 240], [2, 6, 31, 54, 14, 237, 238, 239, 240, 6], [2, 6, 31, 54, 14, 237, 238, 239, 240, 6, 241], [2, 6, 31, 54, 14, 237, 238, 239, 240, 6, 241, 71], [2, 6, 31, 54, 14, 237, 238, 239, 240, 6, 241, 71, 242], [2, 6, 31, 54, 14, 237, 238, 239, 240, 6, 241, 71, 242, 36], [2, 6, 31, 54, 14, 237, 238, 239, 240, 6, 241, 71, 242, 36, 45], [2, 6, 31, 54, 14, 237, 238, 239, 240, 6, 241, 71, 242, 36, 45, 57], [10, 52], [10, 52, 51], [10, 52, 51, 4], [10, 52, 51, 4, 3], [10, 52, 51, 4, 3, 48], [10, 52, 51, 4, 3, 48, 72], [10, 52, 51, 4, 3, 48, 72, 243], [10, 52, 51, 4, 3, 48, 72, 243, 7], [10, 52, 51, 4, 3, 48, 72, 243, 7, 73], [10, 52, 51, 4, 3, 48, 72, 243, 7, 73, 32], [10, 52, 51, 4, 3, 48, 72, 243, 7, 73, 32, 11], [10, 52, 51, 4, 3, 48, 72, 243, 7, 73, 32, 11, 244], [10, 52, 51, 4, 3, 48, 72, 243, 7, 73, 32, 11, 244, 12], [10, 52, 51, 4, 3, 48, 72, 243, 7, 73, 32, 11, 244, 12, 245], [10, 52, 51, 4, 3, 48, 72, 243, 7, 73, 32, 11, 244, 12, 245, 246], [10, 52, 51, 4, 3, 48, 72, 243, 7, 73, 32, 11, 244, 12, 245, 246, 4], [10, 52, 51, 4, 3, 48, 72, 243, 7, 73, 32, 11, 244, 12, 245, 246, 4, 14], [10, 52, 51, 4, 3, 48, 72, 243, 7, 73, 32, 11, 244, 12, 245, 246, 4, 14, 74], [10, 52, 51, 4, 3, 48, 72, 243, 7, 73, 32, 11, 244, 12, 245, 246, 4, 14, 74, 247], [248, 249], [248, 249, 250], [248, 249, 250, 9], [248, 249, 250, 9, 37], [248, 249, 250, 9, 37, 75], [248, 249, 250, 9, 37, 75, 251], [248, 249, 250, 9, 37, 75, 251, 252], [248, 249, 250, 9, 37, 75, 251, 252, 253], [248, 249, 250, 9, 37, 75, 251, 252, 253, 7], [248, 249, 250, 9, 37, 75, 251, 252, 253, 7, 15], [248, 249, 250, 9, 37, 75, 251, 252, 253, 7, 15, 254], [248, 249, 250, 9, 37, 75, 251, 252, 253, 7, 15, 254, 1], [248, 249, 250, 9, 37, 75, 251, 252, 253, 7, 15, 254, 1, 17], [248, 249, 250, 9, 37, 75, 251, 252, 253, 7, 15, 254, 1, 17, 22], [248, 249, 250, 9, 37, 75, 251, 252, 253, 7, 15, 254, 1, 17, 22, 255], [59, 256], [59, 256, 11], [59, 256, 11, 9], [59, 256, 11, 9, 257], [59, 256, 11, 9, 257, 7], [59, 256, 11, 9, 257, 7, 15], [59, 256, 11, 9, 257, 7, 15, 74], [59, 256, 11, 9, 257, 7, 15, 74, 258], [59, 256, 11, 9, 257, 7, 15, 74, 258, 1], [59, 256, 11, 9, 257, 7, 15, 74, 258, 1, 36], [59, 256, 11, 9, 257, 7, 15, 74, 258, 1, 36, 259], [59, 256, 11, 9, 257, 7, 15, 74, 258, 1, 36, 259, 4], [59, 256, 11, 9, 257, 7, 15, 74, 258, 1, 36, 259, 4, 260], [59, 256, 11, 9, 257, 7, 15, 74, 258, 1, 36, 259, 4, 260, 73], [59, 256, 11, 9, 257, 7, 15, 74, 258, 1, 36, 259, 4, 260, 73, 32], [59, 256, 11, 9, 257, 7, 15, 74, 258, 1, 36, 259, 4, 260, 73, 32, 11], [59, 256, 11, 9, 257, 7, 15, 74, 258, 1, 36, 259, 4, 260, 73, 32, 11, 261], [59, 256, 11, 9, 257, 7, 15, 74, 258, 1, 36, 259, 4, 260, 73, 32, 11, 261, 262], [2, 263], [2, 263, 7], [2, 263, 7, 264], [2, 263, 7, 264, 12], [2, 263, 7, 264, 12, 75], [2, 263, 7, 264, 12, 75, 265], [2, 263, 7, 264, 12, 75, 265, 266], [2, 263, 7, 264, 12, 75, 265, 266, 267], [2, 263, 7, 264, 12, 75, 265, 266, 267, 268], [2, 263, 7, 264, 12, 75, 265, 266, 267, 268, 4], [2, 263, 7, 264, 12, 75, 265, 266, 267, 268, 4, 33], [2, 263, 7, 264, 12, 75, 265, 266, 267, 268, 4, 33, 1], [2, 263, 7, 264, 12, 75, 265, 266, 267, 268, 4, 33, 1, 269], [2, 263, 7, 264, 12, 75, 265, 266, 267, 268, 4, 33, 1, 269, 15], [2, 263, 7, 264, 12, 75, 265, 266, 267, 268, 4, 33, 1, 269, 15, 270], [2, 263, 7, 264, 12, 75, 265, 266, 267, 268, 4, 33, 1, 269, 15, 270, 5], [2, 263, 7, 264, 12, 75, 265, 266, 267, 268, 4, 33, 1, 269, 15, 270, 5, 38], [271, 2], [271, 2, 272], [271, 2, 272, 19], [271, 2, 272, 19, 4], [271, 2, 272, 19, 4, 273], [271, 2, 272, 19, 4, 273, 274], [271, 2, 272, 19, 4, 273, 274, 275], [271, 2, 272, 19, 4, 273, 274, 275, 13], [271, 2, 272, 19, 4, 273, 274, 275, 13, 60], [271, 2, 272, 19, 4, 273, 274, 275, 13, 60, 18], [271, 2, 272, 19, 4, 273, 274, 275, 13, 60, 18, 1], [271, 2, 272, 19, 4, 273, 274, 275, 13, 60, 18, 1, 3], [271, 2, 272, 19, 4, 273, 274, 275, 13, 60, 18, 1, 3, 276], [271, 2, 272, 19, 4, 273, 274, 275, 13, 60, 18, 1, 3, 276, 4], [271, 2, 272, 19, 4, 273, 274, 275, 13, 60, 18, 1, 3, 276, 4, 277], [271, 2, 272, 19, 4, 273, 274, 275, 13, 60, 18, 1, 3, 276, 4, 277, 278], [10, 6], [10, 6, 3], [10, 6, 3, 27], [10, 6, 3, 27, 9], [10, 6, 3, 27, 9, 20], [10, 6, 3, 27, 9, 20, 64], [10, 6, 3, 27, 9, 20, 64, 65], [10, 6, 3, 27, 9, 20, 64, 65, 279], [10, 6, 3, 27, 9, 20, 64, 65, 279, 1], [10, 6, 3, 27, 9, 20, 64, 65, 279, 1, 280], [10, 6, 3, 27, 9, 20, 64, 65, 279, 1, 280, 4], [10, 6, 3, 27, 9, 20, 64, 65, 279, 1, 280, 4, 50], [10, 6, 3, 27, 9, 20, 64, 65, 279, 1, 280, 4, 50, 56], [10, 6, 3, 27, 9, 20, 64, 65, 279, 1, 280, 4, 50, 56, 5], [10, 6, 3, 27, 9, 20, 64, 65, 279, 1, 280, 4, 50, 56, 5, 72], [10, 6, 3, 27, 9, 20, 64, 65, 279, 1, 280, 4, 50, 56, 5, 72, 281], [10, 6, 3, 27, 9, 20, 64, 65, 279, 1, 280, 4, 50, 56, 5, 72, 281, 3], [10, 6, 3, 27, 9, 20, 64, 65, 279, 1, 280, 4, 50, 56, 5, 72, 281, 3, 282], [10, 6, 3, 27, 9, 20, 64, 65, 279, 1, 280, 4, 50, 56, 5, 72, 281, 3, 282, 70], [10, 6, 3, 27, 9, 20, 64, 65, 279, 1, 280, 4, 50, 56, 5, 72, 281, 3, 282, 70, 26], [10, 6, 3, 27, 9, 20, 64, 65, 279, 1, 280, 4, 50, 56, 5, 72, 281, 3, 282, 70, 26, 13], [10, 6, 3, 27, 9, 20, 64, 65, 279, 1, 280, 4, 50, 56, 5, 72, 281, 3, 282, 70, 26, 13, 21], [10, 6, 3, 27, 9, 20, 64, 65, 279, 1, 280, 4, 50, 56, 5, 72, 281, 3, 282, 70, 26, 13, 21, 1], [10, 6, 3, 27, 9, 20, 64, 65, 279, 1, 280, 4, 50, 56, 5, 72, 281, 3, 282, 70, 26, 13, 21, 1, 25]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=max([len(x) for x in input_seq])"
      ],
      "metadata": {
        "id": "tLWR9ypLK_xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DByRpju5LCvR",
        "outputId": "ec10a8e6-b3ae-47f8-f88f-f463f16c1546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_seq=pad_sequences(input_seq,maxlen=max_len,padding='pre')"
      ],
      "metadata": {
        "id": "Qh0Qncx4NNiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Rcn1ApSNAJq",
        "outputId": "251512c5-708c-4731-ebbf-a2fc78b15c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0, ...,  0,  2,  6],\n",
              "       [ 0,  0,  0, ...,  2,  6,  3],\n",
              "       [ 0,  0,  0, ...,  6,  3, 76],\n",
              "       ...,\n",
              "       [ 0,  0, 10, ..., 26, 13, 21],\n",
              "       [ 0, 10,  6, ..., 13, 21,  1],\n",
              "       [10,  6,  3, ..., 21,  1, 25]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=padded_input_seq[:,:-1]"
      ],
      "metadata": {
        "id": "B5StMlyHNLG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX0QKNKkNit5",
        "outputId": "66cabfce-e01c-4de3-9755-662a543b433a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(491, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=padded_input_seq[:,-1]"
      ],
      "metadata": {
        "id": "Tky0dGyuNj_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONGn4HS8N1Xy",
        "outputId": "5bb93fee-2fa5-4591-e604-b56708558538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(491, 283)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sbrqO_aN2RW",
        "outputId": "a33f2d12-8b6b-45c4-e172-16913e085577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  6,   3,  76,  77,   5,  21,   1,  25,  41,  78,  16,  42,   8,\n",
              "        79,  17,  22,  26,  42,  80,  18,   6,  81,  12,  10,  82,   8,\n",
              "        83,  10,  84,  27,   1,  85,  27,  86,   7,   2,  28,  23,  17,\n",
              "        22,  11,   1,  87,  88,  89,  90,   1,  44,   6,  45,  46,  91,\n",
              "        29,  92,  13,  93,  24,   1,   3,  94,   5,  95,  30,  96,  47,\n",
              "        31,   3,  48,  97,  32,  49,   9,  98,  99,   1, 100,   4,  50,\n",
              "       101,  11,   6, 103,  29, 104,  51,   6, 105,  52, 106,  23,  46,\n",
              "       107,  47, 108,  31,  53,   3,  17, 109,  54, 110,   4,  14,  49,\n",
              "        13,  33,  24,   6,   3, 111,   5,   2,  30,  55, 112,  11, 113,\n",
              "        23,  56,   5, 114, 115,  57, 116,   1, 117, 118,  13, 119,  24,\n",
              "       121,  16, 122, 123, 124,   5, 125,   4,  34, 126,   3, 128,   5,\n",
              "       129,  28,   6, 130, 131, 132,   4,  34,  24,  29, 133, 134, 135,\n",
              "       136,  58,  16, 137, 138, 139, 140,   2,  35, 141,  14, 142,   9,\n",
              "       143,  20, 144,   7,  18, 146, 147,   1, 148,   8,  14, 149,   5,\n",
              "       150,  36, 151, 152,   5,   2,  19, 153,  44,  12, 154,   1, 155,\n",
              "        58,  16, 156,   1, 157, 158,   8,  15, 159,   4, 160,  33,  13,\n",
              "        60,  18,   6,   3, 161, 162,   5,   2, 163,  61,  11,   4, 164,\n",
              "         9, 165,  62,   7,   2, 166, 167,  15,  39,  12, 168, 169, 170,\n",
              "       171,  63,  20,  64,  65,  35, 172,  23, 173, 174,  39, 175,  62,\n",
              "       176, 177,   7, 178,  40,   1, 179, 180,   8, 181, 182, 183, 184,\n",
              "         8,  66,   9, 185,  10,  37,  67, 186,  40,   5,  25, 187,  39,\n",
              "         1, 188, 190,   3,  67,  20,  68, 191, 192,  40, 193,   2,  12,\n",
              "         3,  61, 194,  20, 196,   2,   8, 197,  35,  55, 198,   1, 199,\n",
              "       200,   9, 201,  21,   1, 202,   2,   4, 204,  30, 205, 206,   4,\n",
              "       207,  17,  22, 208,   1, 209,   7,  69,  70,  41,  16,   3, 210,\n",
              "       212, 213, 214,  63,  43, 215,  66,  68, 216, 217, 218,  69, 220,\n",
              "       221, 222,   2,   1, 223,   7, 224,  26,   8, 225,  53, 226,   6,\n",
              "        14, 227, 228,   5,   2,  19, 230, 231, 232,  71, 233,  28, 234,\n",
              "       235,  15, 236,   6,  31,  54,  14, 237, 238, 239, 240,   6, 241,\n",
              "        71, 242,  36,  45,  57,  52,  51,   4,   3,  48,  72, 243,   7,\n",
              "        73,  32,  11, 244,  12, 245, 246,   4,  14,  74, 247, 249, 250,\n",
              "         9,  37,  75, 251, 252, 253,   7,  15, 254,   1,  17,  22, 255,\n",
              "       256,  11,   9, 257,   7,  15,  74, 258,   1,  36, 259,   4, 260,\n",
              "        73,  32,  11, 261, 262, 263,   7, 264,  12,  75, 265, 266, 267,\n",
              "       268,   4,  33,   1, 269,  15, 270,   5,  38,   2, 272,  19,   4,\n",
              "       273, 274, 275,  13,  60,  18,   1,   3, 276,   4, 277, 278,   6,\n",
              "         3,  27,   9,  20,  64,  65, 279,   1, 280,   4,  50,  56,   5,\n",
              "        72, 281,   3, 282,  70,  26,  13,  21,   1,  25], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y,num_classes=283)"
      ],
      "metadata": {
        "id": "pjn19RUtN33S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8swRJWsLOd25",
        "outputId": "3518a2db-4194-4c81-e24d-1419af686c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=283, output_dim=100))\n",
        "model.add(LSTM(128, return_sequences=False))\n",
        "model.add(Dense(283, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "TQw2q-yfOe3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "pGsfiNI0PeDc",
        "outputId": "0582b906-542f-4436-b579-aee9f6bd204f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y,epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyiKqlZaSER9",
        "outputId": "7ffc8b1f-a3d4-4b45-ceff-b3f399f72ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.0135 - loss: 5.6427\n",
            "Epoch 2/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.0182 - loss: 5.4900\n",
            "Epoch 3/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.0499 - loss: 5.3078\n",
            "Epoch 4/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.0491 - loss: 5.2839\n",
            "Epoch 5/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.0504 - loss: 5.1648\n",
            "Epoch 6/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.0586 - loss: 5.1015\n",
            "Epoch 7/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.0567 - loss: 5.0108\n",
            "Epoch 8/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.0623 - loss: 5.0138\n",
            "Epoch 9/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.0974 - loss: 4.7818\n",
            "Epoch 10/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.0738 - loss: 4.7912\n",
            "Epoch 11/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.0577 - loss: 4.7475\n",
            "Epoch 12/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.0586 - loss: 4.6928\n",
            "Epoch 13/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.0809 - loss: 4.6111\n",
            "Epoch 14/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.0975 - loss: 4.4622\n",
            "Epoch 15/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.0999 - loss: 4.3630\n",
            "Epoch 16/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.1209 - loss: 4.2521\n",
            "Epoch 17/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.1067 - loss: 4.2076\n",
            "Epoch 18/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.1135 - loss: 4.1046\n",
            "Epoch 19/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.1383 - loss: 3.9648\n",
            "Epoch 20/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.1518 - loss: 3.7747\n",
            "Epoch 21/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.1575 - loss: 3.6874\n",
            "Epoch 22/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.1810 - loss: 3.6014\n",
            "Epoch 23/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.2443 - loss: 3.3586\n",
            "Epoch 24/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.2573 - loss: 3.2961\n",
            "Epoch 25/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.3326 - loss: 3.0130\n",
            "Epoch 26/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.3807 - loss: 2.8803\n",
            "Epoch 27/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.3905 - loss: 2.7666\n",
            "Epoch 28/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.4915 - loss: 2.5783\n",
            "Epoch 29/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.5192 - loss: 2.4208\n",
            "Epoch 30/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.5536 - loss: 2.4097\n",
            "Epoch 31/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.6119 - loss: 2.1860\n",
            "Epoch 32/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.6388 - loss: 2.0907\n",
            "Epoch 33/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.7029 - loss: 1.9717\n",
            "Epoch 34/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.7231 - loss: 1.8219\n",
            "Epoch 35/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7333 - loss: 1.8037\n",
            "Epoch 36/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.7755 - loss: 1.6288\n",
            "Epoch 37/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.8195 - loss: 1.4912\n",
            "Epoch 38/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.8130 - loss: 1.4314\n",
            "Epoch 39/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.8314 - loss: 1.3794\n",
            "Epoch 40/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.8650 - loss: 1.3030\n",
            "Epoch 41/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.8649 - loss: 1.2676\n",
            "Epoch 42/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.8825 - loss: 1.1973\n",
            "Epoch 43/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.8965 - loss: 1.0847\n",
            "Epoch 44/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9147 - loss: 1.0226\n",
            "Epoch 45/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9238 - loss: 0.9500\n",
            "Epoch 46/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9286 - loss: 0.9348\n",
            "Epoch 47/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9347 - loss: 0.8696\n",
            "Epoch 48/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9481 - loss: 0.8265\n",
            "Epoch 49/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9476 - loss: 0.7765\n",
            "Epoch 50/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9589 - loss: 0.7251\n",
            "Epoch 51/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9523 - loss: 0.7053\n",
            "Epoch 52/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9662 - loss: 0.6691\n",
            "Epoch 53/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9628 - loss: 0.6195\n",
            "Epoch 54/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9741 - loss: 0.5769\n",
            "Epoch 55/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9677 - loss: 0.5498\n",
            "Epoch 56/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9654 - loss: 0.5312\n",
            "Epoch 57/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9822 - loss: 0.4939\n",
            "Epoch 58/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9742 - loss: 0.5024\n",
            "Epoch 59/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9587 - loss: 0.4809\n",
            "Epoch 60/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9720 - loss: 0.4578\n",
            "Epoch 61/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9742 - loss: 0.4347\n",
            "Epoch 62/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9605 - loss: 0.4257\n",
            "Epoch 63/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9681 - loss: 0.3934\n",
            "Epoch 64/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9721 - loss: 0.3868\n",
            "Epoch 65/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9748 - loss: 0.3825\n",
            "Epoch 66/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9823 - loss: 0.3395\n",
            "Epoch 67/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9724 - loss: 0.3356\n",
            "Epoch 68/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9803 - loss: 0.3173\n",
            "Epoch 69/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9822 - loss: 0.3023\n",
            "Epoch 70/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9711 - loss: 0.3035\n",
            "Epoch 71/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9803 - loss: 0.2839\n",
            "Epoch 72/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9736 - loss: 0.2853\n",
            "Epoch 73/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9752 - loss: 0.2591\n",
            "Epoch 74/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.9803 - loss: 0.2585\n",
            "Epoch 75/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.9822 - loss: 0.2433\n",
            "Epoch 76/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9830 - loss: 0.2399\n",
            "Epoch 77/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9735 - loss: 0.2323\n",
            "Epoch 78/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9638 - loss: 0.2296\n",
            "Epoch 79/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9665 - loss: 0.2234\n",
            "Epoch 80/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9694 - loss: 0.2189\n",
            "Epoch 81/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9700 - loss: 0.2137\n",
            "Epoch 82/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9858 - loss: 0.1855\n",
            "Epoch 83/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9769 - loss: 0.1903\n",
            "Epoch 84/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9718 - loss: 0.1956\n",
            "Epoch 85/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9710 - loss: 0.1899\n",
            "Epoch 86/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9821 - loss: 0.1745\n",
            "Epoch 87/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9743 - loss: 0.1746\n",
            "Epoch 88/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9695 - loss: 0.1839\n",
            "Epoch 89/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9772 - loss: 0.1644\n",
            "Epoch 90/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9770 - loss: 0.1687\n",
            "Epoch 91/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.9827 - loss: 0.1566\n",
            "Epoch 92/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.9838 - loss: 0.1512\n",
            "Epoch 93/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9803 - loss: 0.1577\n",
            "Epoch 94/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9769 - loss: 0.1498\n",
            "Epoch 95/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9682 - loss: 0.1571\n",
            "Epoch 96/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9704 - loss: 0.1492\n",
            "Epoch 97/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9778 - loss: 0.1488\n",
            "Epoch 98/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9808 - loss: 0.1376\n",
            "Epoch 99/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9695 - loss: 0.1444\n",
            "Epoch 100/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9672 - loss: 0.1275\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d588ee95f30>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "text='while talent'\n",
        "for i in range(20):\n",
        "  tokenized_text=tokenizer.texts_to_sequences([text])\n",
        "  padded_text=pad_sequences(tokenized_text,maxlen=max_len,padding='pre')\n",
        "  position_of_softmax=np.argmax([model.predict(padded_text)])\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if position_of_softmax==index:\n",
        "      text=text+' '+word\n",
        "      time.sleep(1)\n",
        "      print(text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PGnaRsUSZfI",
        "outputId": "04151040-963b-4212-b24e-971c45118413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "while talent matters\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "while talent matters effort\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "while talent matters effort is\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "while talent matters effort is crucial\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "while talent matters effort is crucial as\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "while talent matters effort is crucial as it\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "while talent matters effort is crucial as it factors\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "while talent matters effort is crucial as it factors in\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "while talent matters effort is crucial as it factors in twice\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "while talent matters effort is crucial as it factors in twice it\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "while talent matters effort is crucial as it factors in twice it builds\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "while talent matters effort is crucial as it factors in twice it builds skill\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "while talent matters effort is crucial as it factors in twice it builds skill and\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "while talent matters effort is crucial as it factors in twice it builds skill and makes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "while talent matters effort is crucial as it factors in twice it builds skill and makes skill\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "while talent matters effort is crucial as it factors in twice it builds skill and makes skill productive\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "while talent matters effort is crucial as it factors in twice it builds skill and makes skill productive productive\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "while talent matters effort is crucial as it factors in twice it builds skill and makes skill productive productive productive\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "while talent matters effort is crucial as it factors in twice it builds skill and makes skill productive productive productive productive\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "while talent matters effort is crucial as it factors in twice it builds skill and makes skill productive productive productive productive models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gvv5gvMPTmbD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}